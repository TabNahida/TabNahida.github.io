
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>nanozhgpt-dev | Tab&#39;s Blog</title>
    <meta name="author" content="Tab Nahida" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar1.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="https://cdn.jsdelivr.net/npm/mermaid@11.4.1/dist/mermaid.min.js"></script>
<script src="/js/lib/mermaid.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 8.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>TAB&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;TAB&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>nanozhgpt-dev</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2026/2/19
        </span>
        
        <span class="category">
            <a href="/categories/dev-journal/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Dev Journal
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/c-cpp/" style="color: #00a596">
                    C/C++
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/python/" style="color: #ff7d73">
                    Python
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/ai/" style="color: #00a596">
                    AI
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/llm/" style="color: #ff7d73">
                    LLM
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/nlp/" style="color: #00bcd4">
                    NLP
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/model-training/" style="color: #03a9f4">
                    Model Training
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/pytorch/" style="color: #ff7d73">
                    PyTorch
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h1 id="Training-NanoZhGPT-My-Journey-Building-a-100M-Parameter-Chinese-Language-Model"><a href="#Training-NanoZhGPT-My-Journey-Building-a-100M-Parameter-Chinese-Language-Model" class="headerlink" title="Training NanoZhGPT: My Journey Building a 100M Parameter Chinese Language Model"></a>Training NanoZhGPT: My Journey Building a 100M Parameter Chinese Language Model</h1><p>A few weeks ago, I came across a Zhihu article where someone trained a 3M parameter Chinese LLM with decent results. That sparked an idea: why not try training my own? What started as a modest 10M parameter goal eventually evolved into a 100M parameter model.</p>
<span id="more"></span>

<h2 id="Dataset-and-Tokenizer"><a href="#Dataset-and-Tokenizer" class="headerlink" title="Dataset and Tokenizer"></a>Dataset and Tokenizer</h2><p><strong>Dataset</strong>: C4 Chinese portion, ~67GB<br><strong>Tokenizer</strong>: Byte-level BPE, 50K vocabulary</p>
<p>I spent a long time debugging HF’s Tokenizer Trainer. Every run ended the same way: either out of memory or stuck indefinitely during preprocessing. The HF Datasets pipeline would consume massive RAM and never complete on 67GB of data.</p>
<p>After countless failed attempts, I finally decided to write a C++ trainer from scratch—pure vibe coding session. The result: a custom Byte-Level BPE trainer that handles large-scale data efficiently.</p>
<p><strong>Technical details of the C++ trainer:</strong></p>
<ul>
<li>Uses <code>xmake</code> build system with zlib compression support</li>
<li>Reads data paths from <code>.env</code> file (supports json&#x2F;json.gz glob patterns)</li>
<li>Implements chunk-based processing for resume capability</li>
<li>Outputs per-chunk files to <code>chunks/</code> directory</li>
<li>Configurable chunk grouping via <code>--chunk-files N</code></li>
<li>Force rebuild option with <code>--no-resume</code></li>
</ul>
<pre><code class="language-cpp">// Custom C++ Byte-Level BPE Trainer
// Build: xmake f -m release &amp;&amp; xmake
// Run: xmake run byte_bpe_train --output tokenizer.json
// Resume support: chunks/ directory stores intermediate state
</code></pre>
<p>The C++ version completed in a fraction of the time with minimal memory footprint.</p>
<h2 id="Training-Framework"><a href="#Training-Framework" class="headerlink" title="Training Framework"></a>Training Framework</h2><p>When I started this project, I wasn’t familiar with frameworks like PyTorch Lightning. This meant implementing everything from scratch: the training loop, checkpoint saving&#x2F;loading, gradient accumulation, mixed precision, and validation logic.</p>
<p>Building the framework myself gave me deep insights into how these systems work, but it also consumed significant engineering time. The implementation includes:</p>
<ul>
<li>Checkpoint management with metadata tracking</li>
<li>Gradient checkpointing for memory efficiency</li>
<li>Automatic OOM recovery with batch size reduction</li>
<li>XPU&#x2F;CUDA memory monitoring and logging</li>
<li>Streaming data loader with prefetching and RAM caching</li>
</ul>
<pre><code class="language-bash"># Training command example
python train/train_xpu.py \
  --data-dir data/tokens \
  --out-dir checkpoints/gpt100m \
  --seq-len 1024 \
  --n-layer 14 --n-head 10 --n-embd 640 \
  --batch-size 2 --grad-accum 16 \
  --gradient-checkpointing --oom-auto-reduce
</code></pre>
<h2 id="Training-Time-and-Compute"><a href="#Training-Time-and-Compute" class="headerlink" title="Training Time and Compute"></a>Training Time and Compute</h2><p>All models were trained on local XPU except the 300M model, which I’m currently training on rented AutoDL GPU cloud compute.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>Seq Len</th>
<th>Training Time</th>
<th>Compute</th>
</tr>
</thead>
<tbody><tr>
<td>10M</td>
<td>~10M</td>
<td>1024</td>
<td>~8 hours</td>
<td>Local XPU</td>
</tr>
<tr>
<td>100M</td>
<td>~100M</td>
<td>1024</td>
<td>~24 hours</td>
<td>Local XPU</td>
</tr>
<tr>
<td>100M v2</td>
<td>~100M</td>
<td>2048</td>
<td>~36 hours</td>
<td>Local XPU</td>
</tr>
<tr>
<td>300M</td>
<td>~300M</td>
<td>2048</td>
<td>~82 hours*</td>
<td>AutoDL GPU</td>
</tr>
</tbody></table>
<p><em>300M Still in progress</em></p>
<p>The 300M model features extended context length and is my largest training experiment so far.</p>
<h2 id="Training-Results"><a href="#Training-Results" class="headerlink" title="Training Results"></a>Training Results</h2><h3 id="100M-Model-seq-len-1024"><a href="#100M-Model-seq-len-1024" class="headerlink" title="100M Model (seq_len&#x3D;1024)"></a>100M Model (seq_len&#x3D;1024)</h3><pre><code class="language-mermaid">xychart-beta
    title &quot;100M Model: Validation Loss vs Iterations&quot;
    x-axis [500, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 17000]
    y-axis &quot;Validation Loss&quot; 2.5 --&gt; 7.0
    line [6.47, 4.70, 3.86, 3.55, 3.33, 3.24, 3.15, 3.08, 3.02, 3.01]
</code></pre>
<p><strong>Best checkpoint</strong>: Iteration 17,000<br><strong>Best val_loss</strong>: 3.007 (ppl: 20.23)</p>
<h3 id="100M-v2-Model-seq-len-2048"><a href="#100M-v2-Model-seq-len-2048" class="headerlink" title="100M v2 Model (seq_len&#x3D;2048)"></a>100M v2 Model (seq_len&#x3D;2048)</h3><pre><code class="language-mermaid">xychart-beta
    title &quot;100M v2 Model: Validation Loss vs Iterations&quot;
    x-axis [500, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 19000, 20000]
    y-axis &quot;Validation Loss&quot; 3.5 --&gt; 7.5
    line [7.23, 5.66, 4.38, 4.12, 4.03, 3.93, 3.84, 3.79, 3.77, 3.74, 3.73, 3.72]
</code></pre>
<p><strong>Best checkpoint</strong>: Iteration 19,000<br><strong>Best val_loss</strong>: 3.728 (ppl: 41.59)</p>
<p><strong>Note</strong>: Training showed instability with NaN losses at iterations 4,500 and 7,500. Despite doubled sequence length (2048 vs 1024), the model achieved higher validation loss than the original 100M, suggesting training instability or different data sampling affected convergence.</p>
<h3 id="10M-Model-seq-len-1024"><a href="#10M-Model-seq-len-1024" class="headerlink" title="10M Model (seq_len&#x3D;1024)"></a>10M Model (seq_len&#x3D;1024)</h3><pre><code class="language-mermaid">xychart-beta
    title &quot;10M Model: Validation Loss vs Iterations&quot;
    x-axis [500, 2000, 5000, 8000, 10000, 12000, 15000, 17000, 20000]
    y-axis &quot;Validation Loss&quot; 4.0 --&gt; 8.5
    line [8.06, 6.60, 5.41, 5.00, 4.89, 4.81, 4.76, 4.74, 4.72]
</code></pre>
<p><strong>Best checkpoint</strong>: Iteration 20,000<br><strong>Best val_loss</strong>: 4.715 (ppl: 111.65)</p>
<h3 id="Model-Comparison"><a href="#Model-Comparison" class="headerlink" title="Model Comparison"></a>Model Comparison</h3><table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>Seq Len</th>
<th>Best Iter</th>
<th>Val Loss</th>
<th>Perplexity</th>
</tr>
</thead>
<tbody><tr>
<td>100M</td>
<td>~100M</td>
<td>1024</td>
<td>17,000</td>
<td>3.007</td>
<td>20.23</td>
</tr>
<tr>
<td>100M v2</td>
<td>~100M</td>
<td>2048</td>
<td>19,000</td>
<td>3.728</td>
<td>41.59</td>
</tr>
<tr>
<td>10M</td>
<td>~10M</td>
<td>1024</td>
<td>20,000</td>
<td>4.715</td>
<td>111.65</td>
</tr>
</tbody></table>
<h2 id="Key-Observations"><a href="#Key-Observations" class="headerlink" title="Key Observations"></a>Key Observations</h2><ol>
<li><p><strong>Sequence length trade-off</strong>: 100M v2 (seq_len&#x3D;2048) achieved worse loss (3.728) than 100M (seq_len&#x3D;1024, loss 3.007). Training instability with NaN losses at iterations 4.5K and 7.5K suggests longer sequences may require different hyperparameters.</p>
</li>
<li><p><strong>C++ tokenization issue</strong>: After switching to C++ token processing, training performance degraded despite faster data loading.</p>
</li>
<li><p><strong>Loss vs quality gap</strong>: Good validation loss didn’t translate to impressive generation quality.</p>
</li>
<li><p><strong>Training dynamics</strong>: The 10M model showed smoother, more predictable convergence than 100M variants.</p>
</li>
</ol>
<h2 id="Engineering-Insights"><a href="#Engineering-Insights" class="headerlink" title="Engineering Insights"></a>Engineering Insights</h2><p><strong>Pipeline Scalability</strong>: High-level abstractions encountered memory bottlenecks with 67GB+ corpora. A custom C++ tokenizer with chunked I&#x2F;O resolved throughput constraints and enabled resumable preprocessing.</p>
<p><strong>Metric Correlation</strong>: Validation loss convergence did not linearly correlate with generative quality in the &lt;100M regime. Quantitative metrics require supplementation with qualitative evaluation.</p>
<p><strong>Infrastructure Trade-offs</strong>: Custom training loops provide granular control over checkpointing and mixed precision but increase engineering overhead. Established frameworks may better support architecture experimentation.</p>
<p><strong>Scaling Stability</strong>: Transitioning to 100M parameters introduced instability (e.g., NaN losses). Larger configurations necessitate refined hyperparameter tuning, including gradient clipping and learning rate schedules.</p>
<p><strong>Compute Elasticity</strong>: Cloud GPU resources facilitated scaling beyond local XPU limits, highlighting the importance of elastic infrastructure for iterative development.</p>
<h2 id="What’s-Next"><a href="#What’s-Next" class="headerlink" title="What’s Next"></a>What’s Next</h2><p>Planning an English-only model with architectural improvements:</p>
<ul>
<li>Different attention mechanisms</li>
<li>Better normalization strategies  </li>
<li>Longer context lengths</li>
<li>Learning from Lightning and other frameworks</li>
</ul>
<p>The 100M milestone is a good stopping point for this iteration. The 300M model training on AutoDL will provide more insights into scaling. The process of building everything from scratch—from tokenizer to trainer to evaluation—has been incredibly educational.</p>
<p>Sometimes the best way to understand how something works is to build it yourself, bugs and all.</p>
<p>The 100M milestone is a good stopping point. The model won’t win benchmarks, but building it—from tokenizer to trainer to evaluation—taught me more than any tutorial could.</p>
<hr>
<p><strong>Technical Stack</strong>:</p>
<ul>
<li>Tokenizer: Custom C++ Byte-level BPE (50K vocab)</li>
<li>Training: PyTorch on XPU, bfloat16, gradient checkpointing</li>
<li>Dataset: C4 Chinese (~67GB)</li>
<li>Code: Available on GitHub</li>
</ul>

    </div>
    
    
    
    
    <div id="comment">
        <div id="giscus-container" class="giscus"></div>
    </div>
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2024 - 2026 Tab&#39;s Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Tab Nahida
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    
<script
    src="https://giscus.app/client.js"
    data-repo="TabNahida/TabNahida.github.io"
    data-repo-id="R_kgDOJtk4Ag"
    data-category="General"
    data-category-id="DIC_kwDOJtk4As4Ci3B7"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="light_protanopia"
    data-lang="en"
    crossorigin
    async
></script>





    
</body>
</html>
